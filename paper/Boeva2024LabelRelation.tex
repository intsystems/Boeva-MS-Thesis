\documentclass[a4paper, 12pt]{article} %{article}
\usepackage{arxiv}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T2A]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\setlist{nosep}
 

\newcommand{\vecE}{\mathbf{e}}
\newcommand{\vecX}{\mathbf{x}}
\newcommand{\vecY}{\mathbf{y}}


\renewcommand{\abstractname}{Аннотация}


\title{Label Attention Network для последовательной классификации по нескольким меткам.}

\author{ Боева Галина\\
	Антиплагиат\\
	Сколтех\\ 
	\texttt{boeva.gl@phystech.edu} 
	\AND
        Консультант: к.ф.-м.н. Грабовой Андрей\\
	Антиплагиат\\
	\texttt{grabovoy.av@phystech.edu} 
        \AND
        Эксперт: к.ф.-м.н. Зайцев Алексей\\
	Сколтех\\
	\texttt{a.zaytsev@skoltech.ru}
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}
\date{\today}

%\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle

\begin{abstract}
Рассматривается задача прогнозирования временных наборов для последовательных данных. Современные подходы фокусируются на архитектуре преобразования последовательных данных, используя собственное внимания (``self-attention'') к элементам в последовательности. В этом случае учитываются временные взаимодействия событий, но теряется информацию о взаимозависимостях меток. Мотивированные этим недостатком, предлагается использовать механизм собственного внимания(``self-attention'') к меткам, предшествующим прогнозируемому шагу. Поскольку рассматриваемый подход представляет собой сеть внимания к меткам, то называется LANET(Label Attention NETwork). В данном исследовании обосновывается вывод причинно-следственной связи внимания, которое указывает на важность меток и их взаимозависимости.
\end{abstract}


\keywords{временные ряды \and взаимосвязь меток}

\section{Введение}
Классификация с несколькими метками является более естественной, чем бинарная или многоклассовая классификация, поскольку все, что окружает нас в реальном мире, обычно описывается несколькими метками~\cite{liu2021emerging}. Та же логика может быть перенесена на последовательность событий с отметками времени. События в последовательности, как правило, характеризуются несколькими категориальными значениями вместо одной. Существует множество подходов к классификации с несколькими метками в компьютерном зрении~\cite{durand2019learning}, обработке естественного языка~\cite{xiao2019label} или классической структуре табличных данных ~\cite{tarekegn2021review}. Однако постановке задачи с несколькими метками для последовательностей событий, как правило, уделяется меньше внимания. Итак, основной целью является противостоять такому недостатку внимания и решить проблему предсказания набора меток для последовательных данных с временными метками.  
\begin{figure*}
    \centering
    \includegraphics[scale=0.75]{images/nir_inrto.pdf}
    \caption{На рисунке показано визуальное представление постановки задачи. Наша модель должна предсказать метки для момента времени $t_4$, учитывая историю предыдущих наборов меток. Требуется предсказать несколько меток, так что это определение задачи классификации с несколькими метками.}
    \label{fig:nir-intro}
\end{figure*}
Важно отметить, что модель должна предсказывать набор меток, соответствующих следующему шагу, принимая во внимание содержимое предыдущих групп меток для последовательности событий, связанных с объектом(Рисунок~\ref{fig:nir-intro}).

Взаимодействие между состояниями объекта в разные временные метки имеет важное значение для решения задач с последовательными данными~\cite{hartvigsen2020recurrent}. Следовательно, выразительные и мощные модели должны быть способны изучать такие взаимодействия. Несколько архитектур нейронных сетей, таких как трансформеры~\cite{vaswani2017attention} или рекуррентные нейронные сети~\cite{grossberg2013recurrent}, способны делать это. Например, трансформер напрямую определяет механизм внимания, который измеряет, как связаны различные временные метки в последовательности. Однако применение современных методов глубокого обучения ограничено~\cite{zhang2020multi}, и они в первую очередь сосредоточены на прогнозировании меток для последовательности в целом. 

\subsection{Основные подходы для задачи классификации с несколькими метками.}
Постановка задачи классификации с несколькими метками возникает во многих различных областях, например, при категоризации текста или тегировании изображений, и все они влекут за собой свои собственные особенности и проблемы. В обзоре~\cite{zhang2013review} исследуются основы обучения с использованием нескольких меток, обсуждаются хорошо зарекомендовавшие себя методы, а также самые последние подходы. Возникающие тенденции рассматриваются в более свежем обзоре ~\cite{liu2021emerging}.

В работе~\cite{shou2023concurrent} рассматривается та же постановка задачи классификации с несколькими метками в потоке событий, что и у нас. Модель авторов нацелена на фиксацию временных и вероятностных зависимостей между типами параллельных событий путем кодирования исторической информации с помощью энкодера, а затем использования условной смеси экспертов Бернулли. В этой статье~\cite{yu2023continuous} обсуждается постановка задачи прогнозирования временных наборов для пользователей, она предлагает систему непрерывного обучения, которая позволяет явно фиксировать изменяющиеся пользовательские предпочтения, поддерживая банк памяти, который мог бы хранить состояния всех пользователей и элементов. В этой парадигме авторы строят неубывающую универсальную последовательность, содержащую все пользовательские взаимодействия, а затем в хронологическом порядке извлекают уроки из каждого взаимодействия. Для исследования взаимосвязи между продуктами в корзине был предложен ConvTSP~\cite{zhang2023conv}, который объединяет динамические интересы пользователей и статистические интересы в единое векторное представление.

\subsection{Рекомендательные системы.}
В этом разделе мы представим статьи, связанные с проблемой рекомендации следующей корзины. Эта формулировка похожа на нашу, поэтому мы также рассмотрели многие подходы и идеи при анализе нашей области исследований. Авторы в~\cite{ariannezhad2023personalized} предложили персонализированную модель, которая фиксирует краткосрочные зависимости внутри временного набора продуктов, а также долгосрочную, основанную на исторической информации о пользователях. Также в~\cite{yannam2023hybrid} для соединения локальной и глобальной пользовательской информации предлагается гибридный метод, основанный на автоэнкодере~\cite{liou2014autoencoder} для извлечения контекста и рекуррентные нейронные сети для понимания динамики изменения интересов. Чтобы преодолеть подобные проблемы, для предсказания следующей рекомендации создается сеть внимания на основе графов, использующая hyper-edge подход~\cite{song2023hgat}.
При такой постановке задачи возникает сложность работы со словарем товарных категорий, поскольку они насчитывают тысячи значений, в исследовании~\cite{van2023next} используется GRU для прогнозирования следующей корзины, которая легко масштабируется до большого ассортимента.


\paragraph{\textbf{Вклад.}}
Разработана архитектура на основе трансформера на основе собственного внимания между метками для работы над задачей классификации последовательностей событий по нескольким меткам. Основной вклад заключается в следующем:
\begin{itemize}
    \item Была введена архитектура LANET для прогнозирования набора меток для текущего события, используя информацию из предыдущих событий. Особенностью архитектуры является вычисление собственного внимания между представлениями меток.  
    \item LANET превосходит модели на основе трансформера, которые фокусируются на вычислении собственного внимания между временными метками. Оцениваются все показатели в различных наборах данных.
    \item Также построен граф причинно-следственных связей для меток, который использует веса внимания LANET.
\end{itemize}


\section{Постановка задачи}
% Рассматривается задача классификации с несколькими метками для последовательности $S = \{(\bf{X}_{i}, \bf{Y}_{i})\}_{i = 1}^{t-1}$. Он состоит из набора меток $\bf{Y}_{i} \in R^m$, где $m$ --- количество меток в наборе, и набора признаков $\bf{X}_{i}\in R^n$, где $n$ --- количество признаков, специфичных для каждой временной метки от $1$ до $t-1$. Индекс соответствует времени события, поэтому $(bf{X}_{1}, bf{X}_{1})$ --- это информация о первом событии, а $(bf{X}_{t-1}, bf{X}_{t-1})$ - это информация о последнем наблюдаемом событии.
% Множество $Y_i \subseteq \mathcal{Y}$, где $\mathcal{Y} = \{1, 2, \dots, K\}$ --- это множество всех возможных меток. Установленный размер $X_{i}$ равен размеру $Y_{i}$. Каждая метка из $Y_{i}$ сопровождается числовым или категориальным признаком из $X_{i}$ в соответствующей позиции.

% Также может быть дополнительный вектор признаков $\mathbf{z}$, описывающий рассматриваемую последовательность $S$ в целом, например, идентификатор пользователя.
% Цель последовательной классификации с несколькими метками состоит в том, чтобы предсказать набор меток $Y_{t}$ для следующей временной метки.

% Введем функцию $f(\cdot) \in [0, 1]^K$, которая принимает историческую информацию о событиях в качестве входных данных и выводит вектор оценок для каждой из меток $K$. Эти оценки представляют собой вероятности присутствия метки в следующем наборе, связанных с событием.

% В нашей настройке есть ограничение на размер прошлого, доступного модели.
% $S^t = \{(X_{j}, Y_{j})\}_{j = t - \tau}^{t-1}$, где $\tau$ означает количество событий, предшествующих рассматриваемому событию, с отметкой времени $t$, которая равна приписывается целевому набору меток $Y_{t}$.
% Итак, более формально $f(\cdot)$ имеет вид:
% $$
% f(X_{t - \tau}, \ldots, X_{t-1}, Y_{t - \tau}, \ldots, Y_{t-1}, \bf{z}): R^[0, 1]^K
% $$
% для предсказания $Y_{t}$.
В теории последовательности событий каждое событие характеризуется одной категориальной меткой и временной отметкой. Существует множество доступных последовательностей событий, связанных с разными пользователями, с их базовыми моделями развития. При работе с такой структурой данных широко распространенной целью является выявление скрытых закономерностей последовательности на пользовательском и общем уровнях для прогнозирования будущего поведения. В большинстве случаев событию присваивается не одна метка, а некоторый набор меток. Более общей и реалистичной постановкой задачи является рассмотрение возможности одновременной привязки одного и того же момента времени к различным значениям. В дальнейшем будет рассматриваться временные наборы как последовательность связанных с событиями наборов временных меток, состоящих из произвольного числа меток.

Пусть $\mathcal {U}=\{u_1, u_2, \dots, u_N \}$ --- это набор из $N$ элементов.
Каждый элемент $u_i, 1 \leq i \leq N$, связан с последовательностью временных множеств $\mathcal{S}_i = \{s_i^1, s_i^2, \dots, s_i^T\}$, где $T$ --- число наблюдаемых временные метки.  Набор $s_i^j, 1 \leq i \leq N, 1 \leq j \leq T$, представляет собой набор произвольного количества меток, выбранных из словаря $\mathcal{Y}=\{y_1, y_2, \dots, y_L\}$ размера $L$. 

Цель задачи предсказания временных множеств состоит в том, чтобы предсказать последующий набор меток $\hat{s}_i^{T+1}$, то есть,
\begin{equation}
    \hat{s}_i^{T+1} = g(s_i^1, s_i^ 2, \точки, s_i^ T, \mathbf {W}), 
\end{equation} 

где $\mathbf{W}$ относится к обучаемым параметрам функции $g$.

\section{Архитектура LANET}
Большинство моделей, связанных с трансформаторами, используемых для последовательного предсказания с несколькими метками, используют вычисление собственного внимания между последовательными представлениями входных временных меток. 
Вместо этого LANET использует собственное внимание между представлениями меток. 
Пусть $\mathbf{X} \in \mathbb{R}^{L \times D}$ --- матрица представлений всех меток из словаря $\mathcal{Y}=\{y_1, y_2, \dots, y_L\}$. Для каждой временной метки $j, 1 \leq j \leq T$ создается временное представление $\mathbf{t}_j \in \mathbb {R}^{D}$, как это сделано в \cite{shou2023concurrent}:
$$\mathbf{t}_j^{(d)} = \begin{cases}
                       \cos{(t_j / 10000^{\frac{d-1}{D}})}, & \text{if} \; d \; \text{is odd},\\
                       \sin{(t_j / 10000^{\frac{d}{D}})}, & \text{if} \; d \; \text{is even},
\end{cases} $$
Для каждого момента времени $t_j, 1 \leq j \leq T$  образуется матрица представлений $\textbf{Z} \in \mathbb{R}^{L \times D}$. $l$-я строка, $1 \leq l \leq L$, матрицы $\textbf{Z}$, обозначаемая как $\textbf{Z}^{(l, :)}$, равна сумме представлений временных меток, в которых метка $y_l \in \mathcal{Y}$ отображается как элемент набора:
\begin{equation}
    \textbf{Z}^{(l, :)} = \sum_{j | y_l \in s_i^j} \mathbf{t}_j.
\end{equation}  

Объединенное представление последовательности $\mathcal{S}_i = \{s_i^1, s_i^2, \dots, s_i^T\}$ представляет собой объединение определенных матриц, содержащих информацию о времени и структуре набора:
\begin{equation}
    \mathbf{G} = \mathbf{X} \oplus \mathbf{Z}. 
\end{equation} 
Для выявления зависимостей меток $\mathbf {\tilde{G}}$ воспользуемся механизмов собственного внимания:
\begin{equation}
    \mathbf{\tilde{G}}=\text{softmax}(\frac{\mathbf{Q}\mathbf{K^ T}}{\sqrt{2D}})\mathbf{V},
\end{equation} 
где $\mathbf{Q}, \mathbf{K}, \mathbf{V}$ - матрицы запроса, ключа и значения, которые являются линейными преобразованиями матрицы $\mathbf{G}$
После полученных обновленных представлений используется слой предсказаний для получения оценок достоверности для каждой из меток:

\begin{equation}
    \mathbf{\hat{f}} = \text{sigmoid}(\mathbf{\tilde{G}} \mathbf{W}^{\text{out}} + \texbf{b}^{\text{out}}). 
\end{equation} 
LANET обучается end-to-end, принимая историческую последовательность
$\mathcal {S}_i = \{s_i^1, s_i^ 2, \dots, s_i^T\}$ в качестве входных данных и формируя вектор оценки достоверности $\mathbf{\hat{f}}$ в качестве выходных данных. Мы используем следующую функцию потерь:
\begin{equation}
    \mathcal{L}_i = -\frac{1}{L}\sum_{l=1}^L \left(\mathbf{I}_l \log{\mathbf{\hat{f}}}^{(l)} +
\mathbf{I}_l^{'} \log{(1-\mathbf{\hat{f}}}^{(l)}) \right),
\end{equation} 

где $\mathbf{I}_l = \mathbf{I}\{ y_l \in s_i^{T+1} \}$ является индикаторной функцией метки $y_l$, которая является членом множества $s_i^{T+1}$, в то время как $\mathbf{I}_l^{'}$ --- это индикаторная функция с противоположным условием $\mathbf{I}_l^{'}  = \mathbf{I}\{ y_l \notin s_i^{T+1} \}$. Мы обозначим $l$-ю составляющую прогнозируемого вектора оценки достоверности $\mathbf{\hat{f}}$ как $\mathbf{\hat{f}}^{(l)}$.


\section{Вычислительный эксперимент}

\subsection{Описание данных}

\begin{table}[ht!] 
\centering
\caption{Статистика наборов данных для прогнозирования временных наборов.}
\vspace{2pt}
\begin{tabular}{p{1.3cm}p{1.2cm}p{0.8cm}p{0.8cm}p{0.8cm}p{1.0cm}p{0.73cm}}
\hline
Dataset & \#Sets & MdnSS & MaxSS & Vocab & MnLen & \#Seqs  \\
\hline  
Mimic III & 17 849 & 5 & 23  & 169 & 2.7 & 6636 \\ 
Instacart & 115 604 & 6 & 43  & 134 & 16.5 & 7000 \\
\hline
\end{tabular}
\label{tab:multi_datasets}
\end{table}

\begin{itemize}
    \item \textbf{Mimic III} --- датасет, состоящий из медицинских карт пациентов из отделения интенсивной терапии. Событие, связанное с пациентом, включает в себя время поступления в больницу и набор классификационных кодов заболеваний.
    \item \textbf{Instacart} --- набор данных содержит записи о заказах товаров пользователями. Товары из маркетплейсов и магазинов.
\end{itemize}

Общая статистика по рассмотренным наборам данных приведена в таблице~\ref{tab:multi_datasets}.

 

\subsection{Основные результаты}
Показатели для сравнения подхода our PLANET с установленными моделями для решения задачи прогнозирования временных множеств представлены в таблице~\ref{table:diff_data}. LANET демонстрирует высочайшую производительность по всем наборам данных, существенно превосходя своих конкурентов. В Mimic III разброс значений между LANET и другими моделями довольно велик. LANET лучше выявляет взаимозависимости и сложные закономерности в данных. В Mimic III меньше всего событий, что влияет на сходимость моделей. В результате LANET работает быстрее, чем остальные. Более того, если мы рассмотрим набор данных Instacart, который содержит наибольшее количество наборов меток и последовательностей, то качество других подходов будет снижаться из-за сложности структуры набора данных. Подводя итог, можно отметить конкурентоспособность модели TCMBN, которая также основана на архитектуре transformer, но в то же время выигрывает правильная работа с этикетками.  

\begin{table}[ht!] 
\caption{Сравнение подхода our LANET с существующими моделями для прогнозирования временных наборов на основе четырех наборов данных. Выделены наилучшие значения, а вторые по значению подчеркнуты.}
\small
\begin{tabular}{p{0.55cm}cccc}
    \hline
    Data & Model &Weighted F1$\uparrow$ & Weighted ROC-AUC$\uparrow$ & Hamming Loss$\downarrow$\\ \hline 
    \multirow{5}{*}{Mim} 
    & SFCNTSP & 0.3791 $\pm$ 0.0081 & 0.7034 $\pm$ 0.0024 & 0.0377 $\pm$ 0.0004 \\
    & DNNTSP &  0.3928 $\pm$ 0.0030 & 0.6926 $\pm$ 0.0003 & 0.0365 $\pm$ 0.0003\\ 
    & GPTopFreq &  0.4291 $\pm$ 0.0073 & 0.6912 $\pm$ 0.0028 & 0.0398 $\pm$ 0.0005 \\
    & TCMBN &  \underline{0.4979 $\pm$ 0.0180} & \underline{0.8670 $\pm$ 0.0095} & \underline{0.0305 $\pm$ 0.0008} \\ 
    & LANET(ours) &  \textbf{0.8214 $\pm$ 0.0224}   & \textbf{0.9852 $\pm$ 0.0023} & \textbf{0.0220 $\pm$ 0.0001} \\\hline 
    \multirow{5}{*}{Ins}
    & SFCNTSP &  0.1672 $\pm$ 0.0112 & 0.6852 $\pm$ 0.0448 & 0.0581 $\pm$ 0.0004 \\
    & DNNTSP &  \underline{0.4160 $\pm$ 0.0009} & 0.7913 $\pm$ 0.0004 & 0.0541 $\pm$ 0.0002 \\ 
    & GPTopFreq &  0.4087 $\pm$ 0.0079 & 0.7736 $\pm$ 0.0039 & \underline{0.0529 $\pm$ 0.0008}  \\
    & TCMBN &  0.3687 $\pm$ 0.0065 & \underline{0.8187 $\pm$ 0.0030} & 0.0530 $\pm$ 0.0005 \\ 
    & LANET(ours) &\textbf{0.6159 $\pm$ 0.0029}  & \textbf{0.9445 $\pm$ 0.0008} & \textbf{0.0474 $\pm$ 0.0003}\\ \hline
    \end{tabular}
\centering
\label{table:diff_data}
\end{table} 

\subsection{Исследование и интерпретация модели}  

\paragraph{\textbf{Зависимость качества модели от размера представлений.}}

Хотя мы используем обучаемые представления для объектов с метками и временем, необходимо определить размерность их представлений. Для каждого объекта мы создаем представления одинакового размера. Размерность вектора представлений определяет объем информации, которую может воспринять модель, и общее количество изучаемых параметров. Результаты представлены в~\ref{fig:test1_emb}. Заметно, что модель с трудом справляется с эффективным изучением представлений с высокой размерностью.

\paragraph{\textbf{Зависимость качества модели от количества голов во внимании.}}

Гиперпараметром модели является количество голов на уровне внимания. Этот гиперпараметр позволяет модели учитывать различные закономерности в последовательности и концентрироваться на ее отдельных разделах. ~\ref{fig:test2_head} показывает, что с увеличением их количества повышается качество. Но при большом количестве голов потребление ресурсов становится более обременительным, поэтому мы выбрали оптимальный параметр, равный четырем.
 
\paragraph{\textbf{Зависимость производительности модели от количества уровней энкодера.}}

Важным для рассмотрения является гиперпараметр количества уровней энкодера в модели transformer. Этот гиперпараметр отвечает за идентификацию абстрактных представлений, а также за распознавание сложных взаимосвязей между метками. ~\ref{fig:test3_layer} показывает, что мы достигли оптимума при количестве слоев, равном четырем. При дальнейшем улучшении качество снижается, но производительность модели также снижается из-за затухания градиентов.

\paragraph{Графовая интерпретация весов внимания.}
Важной частью результирующей архитектуры является уровень кодирования, который включает в себя уровень внимания. Внимание, в свою очередь, указывает на степень значимости взаимосвязи между метками, которая важна для дальнейшего прогнозирования модели.Мы выбираем наиболее подходящие надписи для подборки в Instacart, чтобы определить причинно-следственные связи между предсказаниями надписей. На рисунке~\ref{fig:ablation2} слева показана тепловая карта для их взаимосвязи. Мы замечаем, что в матрице внимания метки, встречающиеся в последовательности, явно преобладают над теми, которых в ней нет, что четко выражено с помощью весовых коэффициентов. Вглядываясь глубже, мы видим, что небольшие различия во внимании описывают связь между конкретными типами событий. 

Кроме того, мы рассматриваем наиболее релевантные метки для выборки. На рисунке слева показана тепловая карта для их взаимосвязи. Для создания причинно-следственных связей нам потребовалась графическая визуализация шкал внимания для отдельных меток. Эта идея лежит в основе фреймворка CLEANN~\cite{rohekar2024causal}, который предлагает метод извлечения причинно-следственных связей в виде частичного графа предков (PAG) ~\cite{richardson2002ancestral}. Итак, чтобы составить график, мы рассмотрели одного из пользователей и соответствующую историческую информацию о ярлыках. Используя предварительно обученную модель LANET, мы получили значения коэффициента внимания, введенные в алгоритм CLEANN. 

Левая визуализация графика на рисунке~\ref{fig:ablation2} содержит несколько типов связей:
\begin{itemize}  
    \item Красные линии указывают на близость меток внутри графика;
    \item Синие связи более сложны, это двунаправленное взаимодействие между метками на графике;
    \item Черный цвет означает, что метка является родительской для последующих;
    \item Зеленые, напротив, --- это дочерние.
\end{itemize} 
В первом случае между этикетками возникли сложные и запутанные отношения. Например, если исходным кодом является `мясные консервы с морепродуктами", вы создадите `заправки для салатов". Некоторые связи могут показаться нам нелогичными, но эта история индивидуальна для каждого пользователя при покупке товара в магазине. 

Более того, чтобы выяснить и идентифицировать связи, мы решили удалить метку с наибольшим общим весом в матрице внимания и посмотреть на перераспределение весов в этом случае (рисунок~\ref{fig:ablation2} справа). Модель обратила внимание на множество других меток. PAG демонстрирует измененную картинку, на которой исчезли все синие и черные края графика, что соответствует более сложной и ориентированной связи, чем простая `соседская". Корреляция между метками стала ниже. Более того, `мясные консервы с морепродуктами" изменили свое поведение. Она стала дочерней компанией и больше ни с кем не связана, что влияет на прогнозную способность этой этикетки для следующего временного шага. Это исследование показывает, что наилучшие прогностические возможности LANET в основном зависят от способности модели обнаруживать взаимосвязи между ярлыками, а не от построения работы со временем и порядка размещения корзин.
\begin{figure*}
    \centering
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/emb_size.pdf}
    \caption{The dependence of LANET quality on the embedding size.}
    \vspace{10pt}
    \label{fig:test1_emb}
    \end{minipage}\hfill
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/heads_size.pdf}
    \caption{The dependence of LANET quality on the number of heads.}
    \vspace{10pt}
    \label{fig:test2_head}
    \end{minipage}\hfill
    \begin{minipage}{.3\textwidth}
    \centering
    \includegraphics[width=\linewidth]{images/enc_layer.pdf}
    \caption{The dependence of LANET quality on the number of encoder layers.}
    \vspace{10pt}
    \label{fig:test3_layer}
    \end{minipage}
\end{figure*}

\begin{figure*}
%\caption{The dependence of quality on the number}
\centering
\begin{minipage}{.22\textwidth}
\centering
\includegraphics[width=\linewidth]{images/before_graph_var_2.pdf} 
\label{fig:test1}
\end{minipage}
\begin{minipage}{.22\textwidth}
\centering
\includegraphics[width=\linewidth]{images/before_smaller_heatmap.pdf} 
\label{fig:test2}
\end{minipage}%\caption{The dependence of quality on the number}
\begin{minipage}{.22\textwidth}
\centering
\includegraphics[width=\linewidth]{images/after_graph_var_2.pdf} 
\label{fig:test3}
\end{minipage}
\begin{minipage}{.22\textwidth}
\centering
\includegraphics[width=\linewidth]{images/after_smaller_heatmap.pdf} 
\label{fig:test4}
\end{minipage}\caption{Интерпретация взаимосвязи надписей с помощью слоя attention. Слева приведен рисунок, показывающий взаимосвязь между подмножеством надписей и их вербальной интерпретацией. Рядом с графиком приведена тепловая карта, которая иллюстрирует взаимосвязь всех возможных надписей в наборе данных Instacart. Справа представлены измененные графики, которые получены в результате удаления метки с наибольшим весом внимания из всех возможных значений и соответствующего распределения весов на тепловой карте. Данные получены из набора данных Instacart.}
\vspace{10pt}
\label{fig:ablation2}
\end{figure*}

\section{Заключение}
В этой работе рассматривается проблему прогнозирования временных наборов: учитывая историю наборов с временными метками, состоящих из произвольного числа категориальных меток, цель состоит в том, чтобы предсказать набор меток для следующего события. Для решения этой проблемы мы предлагаем модель LANET, который агрегирует историческую информацию в векторные представления, чего нет в других существующих моделях. Особый взгляд на доступную информацию позволяет более эффективно фиксировать временные зависимости и метки. Наш метод демонстрирует наилучшую производительность на четырех рассмотренных наборах данных, превосходя подход SOTA и обеспечивая улучшение на $65\%$ с точки зрения взвешенного F1 на одном из наборов данных. Что касается ограничений, то LANET показывает стабильно высокие результаты, особенно в наборах данных с объемом словаря меток до $200$.
 
\bibliographystyle{unsrt}
\bibliography{references} 

\end{document}
